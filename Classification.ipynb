{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "b56f377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__thể_thao 200\n",
      "__label__chấn_thương 200\n"
     ]
    }
   ],
   "source": [
    "# Thống kê số lượng data theo nhãn\n",
    "count = {}\n",
    "for line in open('texts_categories.txt', encoding=\"utf8\"):\n",
    "    key = line.split()[0]\n",
    "    count[key] = count.get(key, 0) + 1\n",
    "\n",
    "for key in count:\n",
    "    print(key, count[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "81abb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
    "total_label = 3\n",
    "vocab = {}\n",
    "label_vocab = {}\n",
    "for line in open('texts_categories.txt', encoding=\"utf8\"):\n",
    "    words = line.split()\n",
    "    # lưu ý từ đầu tiên là nhãn\n",
    "    label = words[0]\n",
    "    if label not in label_vocab:\n",
    "        label_vocab[label] = {}\n",
    "    for word in words[1:]:\n",
    "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
    "        if word not in vocab:\n",
    "            vocab[word] = set()\n",
    "        vocab[word].add(label)\n",
    "\n",
    "count = {}\n",
    "for word in vocab:\n",
    "    if len(vocab[word]) == total_label:\n",
    "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
    "        \n",
    "sorted_count = sorted(count, key=count.get, reverse=True)\n",
    "for word in sorted_count[:100]:\n",
    "    print(word, count[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "65bc47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loại stopword khỏi dữ liệu\n",
    "# lưu file dùng về sau\n",
    "stopword = set()\n",
    "with open('stopwords.txt', 'w', encoding=\"utf8\") as fp:\n",
    "    for word in sorted_count[:100]:\n",
    "        stopword.add(word)\n",
    "        fp.write(word + '\\n')\n",
    "    \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "    \n",
    "    \n",
    "with open('texts_categories.prep', 'w', encoding=\"utf8\") as fp:\n",
    "    for line in open('texts_categories.txt', encoding=\"utf8\"):\n",
    "        line = remove_stopwords(line)\n",
    "        fp.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "e4993169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__chấn_thương', '__label__thể_thao'] \n",
      "\n",
      "chị hằng nga mẹ bé cho biết trong quá_trình tập_luyện và thi_đấu không ít lần quang_minh gặp chấn_thương trên sân_cỏ tuy rất xót con nhưng khi bé khẳng_định mình không sao cả chị thấy vững_tin về ý_chí con đã trui_rèn được từ thể_thao tôi biết con đá mệt đôi lúc té trên sân rồi bước ra ngoài còn rất đau nhưng tôi vẫn phải vững_lòng để tiếp động_lực giúp con lấy lại tinh_thần thi_đấu chị cũng cảm_thấy vui khi so với tuổi quang_minh khá tự_lập trưởng_thành biết đặt mục_tiêu và kiên_trì chinh_phục ước_mơ theo bóng_đá chuyên_nghiệp trong tương_lai 1 \n",
      "\n",
      "dù khi bị đứt dây_chằng đầu_gối người_bệnh vẫn có_thể đi_lại tuy_nhiên chân bị chấn_thương sẽ khó di_chuyển hơn không_thể uốn_cong và gập đầu_gối như bình_thường một_số người có_thể cảm_nhận tình_trạng lỏng_lẻo ở khớp gối ngoài_ra người_bệnh có_thể nghe thấy tiếng lách_cách hoặc lạo_xạo ở khu_vực bị_thương khớp gối lõm vào hoặc co_thắt cơ 0\n"
     ]
    }
   ],
   "source": [
    "# Chia tập train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "test_percent = 0.2\n",
    "\n",
    "text = []\n",
    "label = []\n",
    "\n",
    "for line in open('texts_categories.prep', encoding=\"utf8\"):\n",
    "    words = line.strip().split()\n",
    "    label.append(words[0])\n",
    "    text.append(' '.join(words[1:]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
    "\n",
    "# Lưu train/test data\n",
    "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
    "with open('train.txt', 'w', encoding=\"utf8\") as fp:\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    "\n",
    "with open('test.txt', 'w', encoding=\"utf8\") as fp:\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        fp.write('{} {}\\n'.format(y, x))\n",
    "\n",
    "# encode label\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print(list(label_encoder.classes_), '\\n')\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(X_train[1], y_train[1], '\\n')\n",
    "print(X_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "61e1e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "4d1f1512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training Logistic Regression in 0.2136993408203125 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs', \n",
    "                                                multi_class='auto',\n",
    "                                                max_iter=10000))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    " \n",
    "train_time = time.time() - start_time\n",
    "print('Done training Logistic Regression in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"lr.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "650c1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training K-Nearest Neighbors in 0.053968191146850586 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', KNeighborsClassifier())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training K-Nearest Neighbors in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"knn.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "67abf711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training Decision Tree Classifier in 0.07795834541320801 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', DecisionTreeClassifier())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Decision Tree Classifier in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"dtc.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "3aaaf956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training Random Forest Classifier in 0.44156670570373535 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Random Forest Classifier in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"rfc.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b7114959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training AdaBoost Classifier in 0.39243125915527344 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', AdaBoostClassifier())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training AdaBoost Classifier in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"abc.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "7b6e85d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training Naive Bayes in 0.15761470794677734 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"nb.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "19bfc04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training SVM in 0.14736008644104004 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
    "                                             max_df=0.8,\n",
    "                                             max_features=None)), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(gamma='scale'))\n",
    "                    ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training SVM in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4e2384d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Accuracy = 0.9875\n",
      "Logistic Regression, Accuracy = 0.975\n",
      "Random Forest Classifier, Accuracy = 0.975\n",
      "K-Nearest Neighbors, Accuracy = 0.9625\n",
      "Naive Bayes, Accuracy = 0.9625\n",
      "AdaBoost Classifier, Accuracy = 0.9125\n",
      "Decision Tree Classifier, Accuracy = 0.8875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SVM\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Logistic Regression\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"lr.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Logistic Regression, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Random Forest Classifier\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"rfc.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Random Forest Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"knn.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('K-Nearest Neighbors, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Naive Bayes\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"nb.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# AdaBoost Classifier\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"abc.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('AdaBoost Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# Decision Tree Classifier\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"dtc.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Decision Tree Classifier, Accuracy =', np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "0de161a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report of SVM:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.97      1.00      0.99        36\n",
      "   __label__thể_thao       1.00      0.98      0.99        44\n",
      "\n",
      "            accuracy                           0.99        80\n",
      "           macro avg       0.99      0.99      0.99        80\n",
      "        weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "Classification Report of LR:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.97      0.97      0.97        36\n",
      "   __label__thể_thao       0.98      0.98      0.98        44\n",
      "\n",
      "            accuracy                           0.97        80\n",
      "           macro avg       0.97      0.97      0.97        80\n",
      "        weighted avg       0.97      0.97      0.97        80\n",
      "\n",
      "Classification Report of RFC:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.95      1.00      0.97        36\n",
      "   __label__thể_thao       1.00      0.95      0.98        44\n",
      "\n",
      "            accuracy                           0.97        80\n",
      "           macro avg       0.97      0.98      0.97        80\n",
      "        weighted avg       0.98      0.97      0.98        80\n",
      "\n",
      "Classification Report of KNN:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.92      1.00      0.96        36\n",
      "   __label__thể_thao       1.00      0.93      0.96        44\n",
      "\n",
      "            accuracy                           0.96        80\n",
      "           macro avg       0.96      0.97      0.96        80\n",
      "        weighted avg       0.97      0.96      0.96        80\n",
      "\n",
      "Classification Report of NB:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.92      1.00      0.96        36\n",
      "   __label__thể_thao       1.00      0.93      0.96        44\n",
      "\n",
      "            accuracy                           0.96        80\n",
      "           macro avg       0.96      0.97      0.96        80\n",
      "        weighted avg       0.97      0.96      0.96        80\n",
      "\n",
      "Classification Report of ABC:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.94      0.86      0.90        36\n",
      "   __label__thể_thao       0.89      0.95      0.92        44\n",
      "\n",
      "            accuracy                           0.91        80\n",
      "           macro avg       0.92      0.91      0.91        80\n",
      "        weighted avg       0.91      0.91      0.91        80\n",
      "\n",
      "Classification Report of DTC:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "__label__chấn_thương       0.94      0.81      0.87        36\n",
      "   __label__thể_thao       0.86      0.95      0.90        44\n",
      "\n",
      "            accuracy                           0.89        80\n",
      "           macro avg       0.90      0.88      0.88        80\n",
      "        weighted avg       0.89      0.89      0.89        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Xem kết quả trên từng nhãn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report of SVM:\")\n",
    "svm_model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of LR:\")\n",
    "lr_model = pickle.load(open(os.path.join(MODEL_PATH,\"lr.pkl\"), 'rb'))\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of RFC:\")\n",
    "rfc_model = pickle.load(open(os.path.join(MODEL_PATH,\"rfc.pkl\"), 'rb'))\n",
    "y_pred_rfc = rfc_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rfc, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of KNN:\")\n",
    "knn_model = pickle.load(open(os.path.join(MODEL_PATH,\"knn.pkl\"), 'rb'))\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_knn, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of NB:\")\n",
    "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"nb.pkl\"), 'rb'))\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_nb, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of ABC:\")\n",
    "abc_model = pickle.load(open(os.path.join(MODEL_PATH,\"abc.pkl\"), 'rb'))\n",
    "y_pred_abc = abc_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_abc, target_names=list(label_encoder.classes_)))\n",
    "\n",
    "print(\"Classification Report of DTC:\")\n",
    "dtc_model = pickle.load(open(os.path.join(MODEL_PATH,\"dtc.pkl\"), 'rb'))\n",
    "y_pred_dtc = dtc_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_dtc, target_names=list(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "eb07188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e777ab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.3.5)\n",
      "Requirement already satisfied: unidecode in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (1.3.4)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (0.9.8)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (1.0.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (3.7)\n",
      "Requirement already satisfied: requests in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (2.27.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (1.1.0)\n",
      "Requirement already satisfied: underthesea-core==0.0.5a2 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (0.0.5a2)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from underthesea) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from Click>=6.0->underthesea) (0.4.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk->underthesea) (2022.10.31)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->underthesea) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->underthesea) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->underthesea) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->underthesea) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->underthesea) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->underthesea) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mypc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->underthesea) (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt một số hàm tiền xử lý văn bản cần thiết\n",
    "!pip3 install --user underthesea\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    " \n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a542a59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict label for \" Trong thể thao, chấn thương là chuyện không thể tránh khỏi của vận động viên \":\n",
      "With SVM: ['__label__chấn_thương']\n",
      "With LR: ['__label__chấn_thương']\n",
      "With RFC: ['__label__chấn_thương']\n",
      "With KNN: ['__label__chấn_thương']\n",
      "With NB: ['__label__chấn_thương']\n",
      "With ABC: ['__label__chấn_thương']\n",
      "With DTC: ['__label__chấn_thương']\n",
      "\n",
      "\n",
      "Predict label for \" Nghĩa đá bóng và chạy bộ rất thường xuyên \":\n",
      "With SVM: ['__label__thể_thao']\n",
      "With LR: ['__label__thể_thao']\n",
      "With RFC: ['__label__thể_thao']\n",
      "With KNN: ['__label__chấn_thương']\n",
      "With NB: ['__label__thể_thao']\n",
      "With ABC: ['__label__thể_thao']\n",
      "With DTC: ['__label__thể_thao']\n",
      "\n",
      "\n",
      "Predict label for \" Nghĩa bị chấn thương khi tập luyện thể thao quá mức \":\n",
      "With SVM: ['__label__chấn_thương']\n",
      "With LR: ['__label__chấn_thương']\n",
      "With RFC: ['__label__chấn_thương']\n",
      "With KNN: ['__label__chấn_thương']\n",
      "With NB: ['__label__chấn_thương']\n",
      "With ABC: ['__label__chấn_thương']\n",
      "With DTC: ['__label__chấn_thương']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_before_predict(document):\n",
    "    document = text_preprocess(document)\n",
    "    document = remove_stopwords(document)\n",
    "    return document\n",
    "\n",
    "def predict_label_lr(document,pred_document):\n",
    "    label = lr_model.predict([pred_document])\n",
    "    print('With LR:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_knn(document,pred_document):\n",
    "    label = knn_model.predict([pred_document])\n",
    "    print('With KNN:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_dtc(document,pred_document):\n",
    "    label = dtc_model.predict([pred_document])\n",
    "    print('With DTC:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_rfc(document,pred_document):\n",
    "    label = rfc_model.predict([pred_document])\n",
    "    print('With RFC:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_abc(document,pred_document):\n",
    "    label = abc_model.predict([pred_document])\n",
    "    print('With ABC:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_nb(document,pred_document):\n",
    "    label = nb_model.predict([pred_document])\n",
    "    print('With NB:', label_encoder.inverse_transform(label))\n",
    "\n",
    "def predict_label_svm(document,pred_document):\n",
    "    label = svm_model.predict([pred_document])\n",
    "    print('With SVM:', label_encoder.inverse_transform(label))\n",
    "\n",
    "documents = []\n",
    "documents.append(\"Trong thể thao, chấn thương là chuyện không thể tránh khỏi của vận động viên\")\n",
    "documents.append(\"Nghĩa đá bóng và chạy bộ rất thường xuyên\")\n",
    "documents.append(\"Nghĩa bị chấn thương khi tập luyện thể thao quá mức\")\n",
    "for i in documents:\n",
    "    print('Predict label for \"',i,'\":')\n",
    "    j = preprocessing_before_predict(i)\n",
    "    predict_label_svm(i,j)\n",
    "    predict_label_lr(i,j)\n",
    "    predict_label_rfc(i,j)\n",
    "    predict_label_knn(i,j)\n",
    "    predict_label_nb(i,j)\n",
    "    predict_label_abc(i,j)\n",
    "    predict_label_dtc(i,j)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
